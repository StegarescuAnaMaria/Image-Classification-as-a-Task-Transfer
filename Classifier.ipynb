{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Classifier.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KnMeI8ow1203"
      },
      "source": [
        "## Environment Setting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xccN7_1QU-ac"
      },
      "source": [
        "!pip install -U torch==1.5 torchvision==0.6 -f https://download.pytorch.org/whl/cu101/torch_stable.html\n",
        "!pip install cython pyyaml==5.1\n",
        "!pip install -U 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'\n",
        "!pip install dominate==2.4.0\n",
        "!pip install detectron2==0.1.2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu101/index.html"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53YjTYjm1j58"
      },
      "source": [
        "## Clone the file from Gihub"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UoxuluzJmbiC"
      },
      "source": [
        "!git clone https://github.com/ericsujw/InstColorization.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zk3WPUbkqSIH",
        "outputId": "bf157350-fff3-4f88-c74f-02333480cb51"
      },
      "source": [
        "cd InstColorization/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/Inst\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L65jjMKVpfkH"
      },
      "source": [
        "## Start Colorization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sO-tvByunH3G"
      },
      "source": [
        "## Download the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S6H_-NktStk_",
        "outputId": "e882e1b6-8177-4d44-ec1a-91522116a395"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UrBzUxrXS9sT"
      },
      "source": [
        "#the path from my Google Drive where everything regarding the project will be saved\n",
        "path = '/content/drive/My Drive/InstColorization'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KXUrhMqTNYwU"
      },
      "source": [
        "!python train.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yIWVsUykTHj4"
      },
      "source": [
        "#paath to test images\n",
        "input_dir = path + '/example'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4rJX5hcmTXTI"
      },
      "source": [
        "from os.path import join, isfile, isdir\n",
        "from os import listdir\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SftEjto3UbMZ"
      },
      "source": [
        "#path to train images\n",
        "train_data = path + '/train_data/train2017'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vlPdgMUUpRzm"
      },
      "source": [
        "!pip install visdom"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2AvlV3O3pGhM"
      },
      "source": [
        "import visdom"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FNAAn6T6YFMt"
      },
      "source": [
        "from torch.nn import init\n",
        "import torch.nn as nn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7quaK94vGwg8"
      },
      "source": [
        "class Siggraph(nn.Module):\n",
        "    def __init__(self, norm_layer=nn.BatchNorm2d):\n",
        "        super(Siggraph, self).__init__()\n",
        "        use_bias = True\n",
        "        # Conv1\n",
        "        model1=[nn.Conv2d(4, 64, kernel_size=3, stride=1, padding=1, bias=use_bias),]\n",
        "        model1+=[nn.ReLU(True),]\n",
        "        model1+=[nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1, bias=use_bias),]\n",
        "        model1+=[norm_layer(64),]\n",
        "        model1+=[nn.ReLU(True),]\n",
        "        \n",
        "        # Conv2\n",
        "        model2=[nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1, bias=use_bias),]\n",
        "        model2+=[nn.ReLU(True),]\n",
        "        model2+=[nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1, bias=use_bias),]\n",
        "        model2+=[norm_layer(128),]\n",
        "        model2+=[nn.ReLU(True),]\n",
        "\n",
        "        # Conv3\n",
        "        model3=[nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1, bias=use_bias),]\n",
        "        model3+=[nn.ReLU(True),]\n",
        "        model3+=[nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1, bias=use_bias),]\n",
        "        model3+=[nn.ReLU(True),]\n",
        "        model3+=[nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1, bias=use_bias),]\n",
        "        model3+=[norm_layer(256),]\n",
        "        model3+=[nn.ReLU(True),]\n",
        "\n",
        "        # Conv4\n",
        "        model4=[nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1, bias=use_bias),]\n",
        "        model4+=[nn.ReLU(True),]\n",
        "        model4+=[nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1, bias=use_bias),]\n",
        "        model4+=[nn.ReLU(True),]\n",
        "        model4+=[nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1, bias=use_bias),]\n",
        "        model4+=[norm_layer(512),]\n",
        "        model4+=[nn.ReLU(True),]\n",
        "        \n",
        "\n",
        "        # Conv5\n",
        "        model5=[nn.Conv2d(512, 512, kernel_size=3, dilation=2, stride=1, padding=2, bias=use_bias),]\n",
        "        model5+=[nn.ReLU(True),]\n",
        "        model5+=[nn.Conv2d(512, 512, kernel_size=3, dilation=2, stride=1, padding=2, bias=use_bias),]\n",
        "        model5+=[nn.ReLU(True),]\n",
        "        model5+=[nn.Conv2d(512, 512, kernel_size=3, dilation=2, stride=1, padding=2, bias=use_bias),]\n",
        "        model5+=[norm_layer(512),]\n",
        "        model5+=[nn.ReLU(True),]\n",
        "        \n",
        "\n",
        "\n",
        "        model_aux6=[nn.Conv2d(512, 256, kernel_size=3, dilation=2, stride=1, padding=2, bias=use_bias),]\n",
        "        model_aux6+=[norm_layer(256),]\n",
        "        model_aux6+=[nn.ReLU(True),]\n",
        "        \n",
        "\n",
        "        model_aux7=[nn.Conv2d(256, 128, kernel_size=3, dilation=2, stride=1, padding=2, bias=use_bias),]\n",
        "        model_aux7+=[norm_layer(128),]\n",
        "        model_aux7+=[nn.ReLU(True),]\n",
        "        \n",
        "\n",
        "        model_aux8=[nn.Conv2d(128, 64, kernel_size=3, dilation=2, stride=1, padding=2, bias=use_bias),]\n",
        "        model_aux8+=[norm_layer(64),]\n",
        "        model_aux8+=[nn.ReLU(True),]\n",
        "        \n",
        "\n",
        "        model_aux9=[nn.Conv2d(64, 32, kernel_size=3, dilation=2, stride=1, padding=2, bias=use_bias),]\n",
        "        model_aux9+=[norm_layer(32),]\n",
        "        model_aux9+=[nn.ReLU(True),]\n",
        "        \n",
        "\n",
        "        model_aux10=[nn.Linear(in_features = 32768, out_features = 10),]\n",
        "\n",
        "        self.model1 = nn.Sequential(*model1)\n",
        "        self.model2 = nn.Sequential(*model2)\n",
        "        self.model3 = nn.Sequential(*model3)\n",
        "        self.model4 = nn.Sequential(*model4)\n",
        "        self.model5 = nn.Sequential(*model5)\n",
        "        self.model_aux6 = nn.Sequential(*model_aux6)\n",
        "        self.model_aux7 = nn.Sequential(*model_aux7)\n",
        "        self.model_aux8 = nn.Sequential(*model_aux8)\n",
        "        self.model_aux9 = nn.Sequential(*model_aux9)\n",
        "        self.model_aux10 = nn.Sequential(*model_aux10)\n",
        "\n",
        "\n",
        "    def forward(self, input_A, input_B, mask_B):\n",
        "        conv1_2 = self.model1(torch.cat((input_A,input_B,mask_B),dim=1))\n",
        "        conv2_2 = self.model2(conv1_2[:,:,::2,::2])\n",
        "        conv3_3 = self.model3(conv2_2[:,:,::2,::2])\n",
        "        conv4_3 = self.model4(conv3_3[:,:,::2,::2])\n",
        "        conv5_3 = self.model5(conv4_3)\n",
        "        conv6 = self.model_aux6(conv5_3)\n",
        "        conv7 = self.model_aux7(conv6)\n",
        "        conv8 = self.model_aux8(conv7)\n",
        "        conv9 = self.model_aux9(conv8)\n",
        "        conv9 = conv9.view(25, -1)\n",
        "        conv10 = self.model_aux10(conv9)\n",
        "        \n",
        "        return conv10\n",
        "\n",
        "\n",
        "def init_net(net, init_type='xavier'):\n",
        "    net = torch.nn.DataParallel(net)\n",
        "    init_weights(net, init_type)\n",
        "    return net\n",
        "\n",
        "\n",
        "def init_weights(net, init_type='xavier', gain=0.02):\n",
        "    def init_func(m):\n",
        "        classname = m.__class__.__name__\n",
        "        if hasattr(m, 'weight') and (classname.find('Conv') != -1 or classname.find('Linear') != -1):\n",
        "            if init_type == 'normal':\n",
        "                init.normal_(m.weight.data, 0.0, gain)\n",
        "            elif init_type == 'xavier':\n",
        "                init.xavier_normal_(m.weight.data, gain=gain)\n",
        "            elif init_type == 'kaiming':\n",
        "                init.kaiming_normal_(m.weight.data, a=0, mode='fan_in')\n",
        "            elif init_type == 'orthogonal':\n",
        "                init.orthogonal_(m.weight.data, gain=gain)\n",
        "            else:\n",
        "                raise NotImplementedError('initialization method [%s] is not implemented' % init_type)\n",
        "            if hasattr(m, 'bias') and m.bias is not None:\n",
        "                init.constant_(m.bias.data, 0.0)\n",
        "        elif classname.find('BatchNorm2d') != -1:\n",
        "            init.normal_(m.weight.data, 1.0, gain)\n",
        "            init.constant_(m.bias.data, 0.0)\n",
        "\n",
        "    print('initialize network with %s' % init_type)\n",
        "    net.apply(init_func)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0hLf3qQ74DcF"
      },
      "source": [
        "#path_full = '/content/drive/My Drive/InstColorization/checkpoints/instance/coco_mask/latest_net_G.pth'\n",
        "#path_instance = '/content/drive/My Drive/InstColorization/checkpoints/full/coco_mask/latest_net_G.pth'\n",
        "path_instance = '/content/drive/My Drive/check_full/new/latest_net_G.pth'\n",
        "#path_instance = '/content/drive/My Drive/checkpoints2/coco_mask/latest_net_G.pth'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxAGgHnGsWTn"
      },
      "source": [
        "#saves data about the objects detected with Detectron2\n",
        "!python inference_bbox.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bnu7AoiL5UC9"
      },
      "source": [
        "import torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZp_6fFN7oJZ"
      },
      "source": [
        "class MyModelInst():\n",
        "    def __init__(self):\n",
        "        #self.netG = init_net(Siggraph(), 'xavier')\n",
        "        self.netG = Siggraph()\n",
        "        #self.netG = Siggraph()\n",
        "        #self.netG.init_net(self.netG, 'xavier')\n",
        "        self.loss = torch.nn.CrossEntropyLoss()\n",
        "        self.mask_cent = 0.5\n",
        "\n",
        "    # set requies_grad=Fasle to avoid computation\n",
        "    def set_requires_grad(self, nets, requires_grad=False):\n",
        "        if not isinstance(nets, list):\n",
        "            nets = [nets]\n",
        "        for net in nets:\n",
        "            if net is not None:\n",
        "                for param in net.parameters():\n",
        "                    param.requires_grad = requires_grad\n",
        "\n",
        "\n",
        "    def optimize_parameters(self, target):\n",
        "        pred = self.forward()\n",
        "        #acc = torch.sum(torch.argmax(target, dim=1)==torch.argmax(pred.to(torch.device('cpu')), dim=1))\n",
        "        #pred.to(torch.device('cuda:0'))\n",
        "        #acc = torch.sum(torch.argmax(target.to(torch.device('cuda')), dim=1)==torch.argmax(pred, dim=1))\n",
        "        acc = torch.sum(target == torch.argmax(pred, dim=1))\n",
        "        loss = self.loss(pred, target)\n",
        "        \n",
        "        loss.backward()\n",
        "        self.optimizer_G.step()\n",
        "        self.optimizer_G.zero_grad()\n",
        "\n",
        "        return loss, acc\n",
        "\n",
        "    def forward(self):\n",
        "        pred = self.netG(self.real_A, self.hint_B, self.mask_B)\n",
        "        return pred\n",
        "\n",
        "    def set_input(self, input):\n",
        "        AtoB = self.opt.which_direction == 'AtoB'\n",
        "        self.real_A = input['A' if AtoB else 'B']\n",
        "        self.real_B = input['B' if AtoB else 'A']\n",
        "        self.hint_B = input['hint_B']\n",
        "        \n",
        "        self.mask_B = input['mask_B']\n",
        "        self.mask_B_nc = self.mask_B + self.mask_cent\n",
        "\n",
        "        self.real_B_enc = util.encode_ab_ind(self.real_B[:, :, ::4, ::4], self.opt)\n",
        "\n",
        "    def save_networks(self, which_epoch):\n",
        "          name = 'G'\n",
        "          save_filename = '%s_net_%s.pth' % (which_epoch, name)\n",
        "          save_path = os.path.join('/content/drive/My Drive/check_full/new', save_filename)    #path to checkpoints for the classification-full model\n",
        "          net = self.netG\n",
        "          torch.save(net.cpu().state_dict(), save_path)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I50-HUBf7r0J"
      },
      "source": [
        "loaded_state_dict = torch.load(path_instance)\n",
        "\n",
        "\n",
        "my_model_inst = MyModelInst()\n",
        "\n",
        "own_state = my_model_inst.netG.state_dict()\n",
        "for name, param in loaded_state_dict.items():\n",
        "    if name not in own_state:\n",
        "        continue\n",
        "    param = param.data\n",
        "    own_state[name].copy_(param)\n",
        "\n",
        "my_model_inst.netG.load_state_dict(own_state)\n",
        "\n",
        "for idx, param in enumerate(my_model_inst.netG.parameters()):\n",
        "  if(idx<51):\n",
        "    param.requires_grad = False\n",
        "\n",
        "my_model_inst.optimizer_G = torch.optim.Adam(my_model_inst.netG.parameters(),\n",
        "                                                lr=0.0001, betas=(0.9, 0.999))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6lvEwwuon4Of"
      },
      "source": [
        "import torch.utils.data as Data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYq7RUHBOldI"
      },
      "source": [
        "import image_util\n",
        "import importlib\n",
        "import util\n",
        "importlib.reload(image_util)\n",
        "importlib.reload(util)\n",
        "from util import util\n",
        "from image_util import *\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "from random import sample\n",
        "\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UIkQm-WSZLzT"
      },
      "source": [
        "class Training_Instance_Dataset(Data.Dataset):\n",
        "    '''\n",
        "    Training on COCOStuff dataset. [train2017.zip]\n",
        "    \n",
        "    Download the training set from https://github.com/nightrome/cocostuff\n",
        "\n",
        "    Make sure you've predicted all the images' bounding boxes using inference_bbox.py\n",
        "\n",
        "    It would be better if you can filter out the images which don't have any box.\n",
        "    '''\n",
        "    def __init__(self):\n",
        "        self.PRED_BBOX_DIR = '/content/drive/My Drive/train_bbox'\n",
        "        self.IMAGE_DIR = '/content/drive/My Drive/train'\n",
        "        self.IMAGE_ID_LIST = [f for f in listdir(self.IMAGE_DIR) if isfile(join(self.IMAGE_DIR, f))]\n",
        "        self.transforms = transforms.Compose([\n",
        "            transforms.Resize((256, 256), interpolation=2),\n",
        "            transforms.ToTensor()\n",
        "        ])\n",
        "        self.labels_train = torch.zeros(994, dtype = torch.int64)\n",
        "        self.labels_train[:95] = 1\n",
        "        self.labels_train[95:195] = 9\n",
        "        self.labels_train[195:294] = 3\n",
        "        self.labels_train[294:394] = 5\n",
        "        self.labels_train[394:494] = 6\n",
        "        self.labels_train[494:594] = 4\n",
        "        self.labels_train[594:694] = 2\n",
        "        self.labels_train[694:794] = 7\n",
        "        self.labels_train[794:894] = 0\n",
        "        self.labels_train[794:994] = 8\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        pred_info_path = join(self.PRED_BBOX_DIR, self.IMAGE_ID_LIST[index].split('.')[0] + '.npz')\n",
        "        output_image_path = join(self.IMAGE_DIR, self.IMAGE_ID_LIST[index])\n",
        "        pred_bbox = gen_maskrcnn_bbox_fromPred(pred_info_path)\n",
        "\n",
        "        rgb_img, gray_img = gen_gray_color_pil(output_image_path)\n",
        "        #print('here')\n",
        "        index_list = range(len(pred_bbox))\n",
        "        if index_list !=range(0, 0):\n",
        "          index_list = sample(index_list, 1)\n",
        "          startx, starty, endx, endy = pred_bbox[index_list[0]]\n",
        "          output = {}\n",
        "          output['rgb_img'] = self.transforms(rgb_img.crop((startx, starty, endx, endy)))\n",
        "          output['gray_img'] = self.transforms(gray_img.crop((startx, starty, endx, endy)))\n",
        "        else:\n",
        "          output = {}\n",
        "          output['rgb_img'] = self.transforms(rgb_img)\n",
        "          output['gray_img'] = self.transforms(gray_img)\n",
        "        output['label'] = self.labels_train[index]\n",
        "        return output\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.IMAGE_ID_LIST)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mA-2p1Nq0FJx"
      },
      "source": [
        "from torchvision import transforms"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IWtqkHJYcqi-"
      },
      "source": [
        "dataset = Training_Instance_Dataset()\n",
        "dataset_loader = torch.utils.data.DataLoader(dataset, batch_size=25, shuffle=True, num_workers=8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gkMgY9PTi_Cn"
      },
      "source": [
        "from tqdm import trange, tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_m_ahV7UzUXo"
      },
      "source": [
        "import sre_compile"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PXEohUzDtKyb"
      },
      "source": [
        "class Opt():\n",
        "  def __init__(self):\n",
        "      self.l_norm = 100.\n",
        "      self.l_cent = 50.\n",
        "      self.ab_norm = 110.\n",
        "      self.ab_max = 110. \n",
        "      self.ab_quant = 10.\n",
        "      self.mask_cent = .5\n",
        "      self.batch_size = 25\n",
        "      self.loadSize = 256\n",
        "      self.fineSize = 256\n",
        "      self.input_nc = 1\n",
        "      self.output_nc = 2\n",
        "      self.ngf = 64\n",
        "      self.ndf = 64\n",
        "      self.which_model_netD = 'basic'\n",
        "      self.which_model_netG = 'siggraph'\n",
        "      self.n_layers_D = 3\n",
        "      self.dataset_mode = 'aligned'\n",
        "      self.which_direction ='AtoB'\n",
        "      self.nThreads = 4\n",
        "      #self.chekpoints_dir = '/content/drive/My Drive/InstColorization/checkpoints/instance/coco_mask/latest_net_G.pth'\n",
        "      self.norm = 'batch'\n",
        "      self.sample_p = 1.0\n",
        "      self.sample_Ps = [1, 2, 3, 4, 5, 6, 7, 8, 9, ]\n",
        "      self.aspect_ratio = 1.0\n",
        "      self.A = sre_compile.SRE_FLAG_ASCII"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "muuslNWFJifV"
      },
      "source": [
        "#train instance model\n",
        "batch_size=25\n",
        "accuracy = 0\n",
        "total_steps = 0\n",
        "opt = Opt()\n",
        "my_model_inst.opt = opt\n",
        "niter = 0\n",
        "for_plot = []\n",
        "#my_model_inst.set_requires_grad(my_model_inst.netG, True)\n",
        "for epoch in trange(0, 100, desc='epoch', dynamic_ncols=True):\n",
        "      epoch_iter = 0\n",
        "      accuracy_total = 0\n",
        "      for data_raw in tqdm(dataset_loader, desc='batch', dynamic_ncols=True, leave=False):\n",
        "            total_steps += batch_size\n",
        "            epoch_iter += batch_size\n",
        "\n",
        "            data_raw['rgb_img'] = [data_raw['rgb_img']]\n",
        "            data_raw['gray_img'] = [data_raw['gray_img']]\n",
        "\n",
        "            input_data = util.get_colorization_data(data_raw['gray_img'], opt, p=1.0, ab_thresh=0)\n",
        "            gt_data = util.get_colorization_data(data_raw['rgb_img'], opt, p=1.0, ab_thresh=10.0)\n",
        "            if gt_data is None:\n",
        "                continue\n",
        "            if(gt_data['B'].shape[0] < batch_size):\n",
        "                continue\n",
        "            input_data['B'] = gt_data['B']\n",
        "            input_data['hint_B'] = gt_data['hint_B']\n",
        "            input_data['mask_B'] = gt_data['mask_B']\n",
        "            niter += 1\n",
        "            my_model_inst.set_input(input_data)\n",
        "            loss, acc = my_model_inst.optimize_parameters(data_raw['label'])\n",
        "            accuracy_total += float(acc)/float(batch_size)\n",
        "            accuracy = float(acc)/float(batch_size)\n",
        "            for_plot.append(100.0*accuracy)\n",
        "            if epoch % 5 == 0:\n",
        "                print(\"Epoch = {0} Loss = {1} Accuracy = {2}\".format(epoch, loss, 100*accuracy))\n",
        "            my_model_inst.save_networks('latest')\n",
        "            my_model_inst.save_networks(epoch)\n",
        "            #my_model_inst.update_learning_rate()\n",
        "      acc = 100*accuracy_total/niter\n",
        "      print(\"Total accuracy: {0}\".format(acc))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jUJU5Yy7LPlm",
        "outputId": "e418dcba-0921-4a3a-9a16-947f5f301fc3"
      },
      "source": [
        "!python inference_bbox.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " 30% 30/99 [00:08<00:20,  3.37it/s]libpng warning: iCCP: known incorrect sRGB profile\n",
            "100% 99/99 [00:33<00:00,  2.91it/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IX_AxWjcPqb9"
      },
      "source": [
        "import importlib\n",
        "import Inst"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ijnaahGhKAJB"
      },
      "source": [
        "class Testing_Instance_Dataset(Data.Dataset):\n",
        "    '''\n",
        "    Training on COCOStuff dataset. [train2017.zip]\n",
        "    \n",
        "    Download the training set from https://github.com/nightrome/cocostuff\n",
        "\n",
        "    Make sure you've predicted all the images' bounding boxes using inference_bbox.py\n",
        "\n",
        "    It would be better if you can filter out the images which don't have any box.\n",
        "    '''\n",
        "    def __init__(self):\n",
        "        self.PRED_BBOX_DIR = '/content/drive/My Drive/train_bbox/test_bbox'\n",
        "        self.IMAGE_DIR = '/content/drive/My Drive/train_bbox/test'\n",
        "        self.IMAGE_ID_LIST = [f for f in listdir(self.IMAGE_DIR) if isfile(join(self.IMAGE_DIR, f))]\n",
        "        self.transforms = transforms.Compose([\n",
        "            transforms.Resize((256, 256), interpolation=2),\n",
        "            transforms.ToTensor()\n",
        "        ])\n",
        "\n",
        "        self.labels_train = torch.zeros(100, dtype = torch.int64)\n",
        "        self.labels_train[:10] = 1\n",
        "        self.labels_train[10:20] = 9\n",
        "        self.labels_train[20:30] = 3\n",
        "        self.labels_train[30:40] = 5\n",
        "        self.labels_train[40:50] = 6\n",
        "        self.labels_train[50:60] = 4\n",
        "        self.labels_train[60:70] = 2\n",
        "        self.labels_train[70:80] = 7\n",
        "        self.labels_train[80:90] = 0\n",
        "        self.labels_train[90:100] = 8\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        pred_info_path = join(self.PRED_BBOX_DIR, self.IMAGE_ID_LIST[index].split('.')[0] + '.npz')\n",
        "        output_image_path = join(self.IMAGE_DIR, self.IMAGE_ID_LIST[index])\n",
        "        pred_bbox = gen_maskrcnn_bbox_fromPred(pred_info_path)\n",
        "\n",
        "        rgb_img, gray_img = gen_gray_color_pil(output_image_path)\n",
        "        index_list = range(len(pred_bbox))\n",
        "        if index_list !=range(0, 0):\n",
        "          index_list = sample(index_list, 1)\n",
        "          startx, starty, endx, endy = pred_bbox[index_list[0]]\n",
        "          output = {}\n",
        "          output['rgb_img'] = self.transforms(rgb_img.crop((startx, starty, endx, endy)))\n",
        "          output['gray_img'] = self.transforms(gray_img.crop((startx, starty, endx, endy)))\n",
        "        else:\n",
        "          output = {}\n",
        "          output['rgb_img'] = self.transforms(rgb_img)\n",
        "          output['gray_img'] = self.transforms(gray_img)\n",
        "        output['label'] = self.labels_train[index]\n",
        "        return output\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.IMAGE_ID_LIST)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GuelzngLRMmb"
      },
      "source": [
        "dataset = Testing_Instance_Dataset()\n",
        "dataset_loader = torch.utils.data.DataLoader(dataset, batch_size=25, shuffle=True, num_workers=8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JdkXee3ZTQRK"
      },
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qdHkOA9NTG2s"
      },
      "source": [
        "def plot_res(x, y, color='blue', title = ''):\n",
        "    plt.plot(x, y, '-', color=color)\n",
        "    plt.title(title)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1krv_iBPTg4c"
      },
      "source": [
        "path_loss = '/content/drive/My Drive/InstColorization/checkpoints/plot/loss_log.txt'    #path to text log loss file"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}